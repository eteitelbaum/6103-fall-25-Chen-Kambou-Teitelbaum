{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e4a177",
   "metadata": {},
   "source": [
    "# FLFP Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c09b38",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd1028e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For data splitting and preprocessing\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For modeling and evaluation\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc8b092",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03607580",
   "metadata": {},
   "source": [
    "### Load the FLFP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "122bad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5208 entries, 0 to 5207\n",
      "Data columns (total 25 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   country_name             5208 non-null   object \n",
      " 1   flfp_15_64               4484 non-null   float64\n",
      " 2   year                     5208 non-null   int64  \n",
      " 3   fertility_rate           5208 non-null   float64\n",
      " 4   fertility_adolescent     5208 non-null   float64\n",
      " 5   urban_population         5160 non-null   float64\n",
      " 6   dependency_ratio         5208 non-null   float64\n",
      " 7   life_exp_female          5208 non-null   float64\n",
      " 8   infant_mortality         4704 non-null   float64\n",
      " 9   population_total         5208 non-null   float64\n",
      " 10  secondary_enroll_fe      3427 non-null   float64\n",
      " 11  tertiary_enroll_fe       2979 non-null   float64\n",
      " 12  gender_parity_primary    2874 non-null   float64\n",
      " 13  gender_parity_secondary  2934 non-null   float64\n",
      " 14  gdp_per_capita_const     4935 non-null   float64\n",
      " 15  gdp_growth               4957 non-null   float64\n",
      " 16  services_gdp             4635 non-null   float64\n",
      " 17  industry_gdp             4684 non-null   float64\n",
      " 18  rule_of_law              4683 non-null   float64\n",
      " 19  unemployment_total       4484 non-null   float64\n",
      " 20  unemployment_female      4484 non-null   float64\n",
      " 21  labor_force_total        4484 non-null   float64\n",
      " 22  iso3c                    5208 non-null   object \n",
      " 23  region                   5208 non-null   object \n",
      " 24  income_level             5208 non-null   object \n",
      "dtypes: float64(20), int64(1), object(4)\n",
      "memory usage: 1017.3+ KB\n"
     ]
    }
   ],
   "source": [
    "flfp_df = pd.read_parquet('data/flfp_dataset.parquet')\n",
    "flfp_df['region'] = flfp_df['region'].str.strip()  # Clean trailing spaces\n",
    "flfp_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d365dac",
   "metadata": {},
   "source": [
    "### Perform train-test split\n",
    "With this being panel data, we have to be careful not to leak future data for any country into the training set. \n",
    "\n",
    "In this notebook, we will do a time-based split, using early years for training and later years for validation and testing. This creates a forecasting-style problem where the model sees earlier years for each country and is evaluated on later years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd525abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-based (temporal) Train/Validation/Test Split\n",
      "==================================================\n",
      "Years in data: [2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013\n",
      " 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023]\n",
      "Number of distinct years: 24\n",
      "\n",
      "Train years (19): [2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013\n",
      " 2014 2015 2016 2017 2018]\n",
      "Validation years (2): [2019 2020]\n",
      "Test years (3): [2021 2022 2023]\n",
      "\n",
      "Training observations: 3,553 (79.2%)\n",
      "Validation observations: 374 (8.3%)\n",
      "Test observations: 557 (12.4%)\n",
      "\n",
      "✓ Time-based split created successfully (80/10/10 by year)\n"
     ]
    }
   ],
   "source": [
    "# Filter to FLFP observations\n",
    "modeling_df = flfp_df[flfp_df['flfp_15_64'].notna()].copy()\n",
    "\n",
    "print(\"Time-based (temporal) Train/Validation/Test Split\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Inspect years\n",
    "years = np.sort(modeling_df['year'].unique())\n",
    "n_years = len(years)\n",
    "print(f\"Years in data: {years}\")\n",
    "print(f\"Number of distinct years: {n_years}\")\n",
    "\n",
    "# Define 80/10/10 split by year (earliest → latest)\n",
    "train_end_idx = int(np.floor(0.8 * n_years))        # end (exclusive) of train years\n",
    "val_end_idx   = int(np.floor(0.9 * n_years))        # end (exclusive) of train+val years\n",
    "\n",
    "train_years = years[:train_end_idx]\n",
    "val_years   = years[train_end_idx:val_end_idx]\n",
    "test_years  = years[val_end_idx:]\n",
    "\n",
    "print(f\"\\nTrain years ({len(train_years)}): {train_years}\")\n",
    "print(f\"Validation years ({len(val_years)}): {val_years}\")\n",
    "print(f\"Test years ({len(test_years)}): {test_years}\")\n",
    "\n",
    "# Create boolean masks based on year\n",
    "train_mask = modeling_df['year'].isin(train_years)\n",
    "val_mask   = modeling_df['year'].isin(val_years)\n",
    "test_mask  = modeling_df['year'].isin(test_years)\n",
    "\n",
    "# Subset the main dataframe\n",
    "train_df = modeling_df[train_mask].copy()\n",
    "val_df   = modeling_df[val_mask].copy()\n",
    "test_df  = modeling_df[test_mask].copy()\n",
    "\n",
    "print(f\"\\nTraining observations: {len(train_df):,} ({len(train_df)/len(modeling_df)*100:.1f}%)\")\n",
    "print(f\"Validation observations: {len(val_df):,} ({len(val_df)/len(modeling_df)*100:.1f}%)\")\n",
    "print(f\"Test observations: {len(test_df):,} ({len(test_df)/len(modeling_df)*100:.1f}%)\")\n",
    "\n",
    "# Optional: check that every year is assigned to exactly one set\n",
    "assert set(train_years).isdisjoint(val_years)\n",
    "assert set(train_years).isdisjoint(test_years)\n",
    "assert set(val_years).isdisjoint(test_years)\n",
    "\n",
    "time_split = {\n",
    "    'train_years': train_years,\n",
    "    'val_years': val_years,\n",
    "    'test_years': test_years,\n",
    "    'train_df': train_df,\n",
    "    'val_df': val_df,\n",
    "    'test_df': test_df,\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Time-based split created successfully (80/10/10 by year)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b523301",
   "metadata": {},
   "source": [
    "### Add categorical features (region and income level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "836a9af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out 'Not classified' income observations...\n",
      "  Train: 3553 → 3515 (-38)\n",
      "  Val: 374 → 370 (-4)\n",
      "  Test: 557 → 551 (-6)\n",
      "\n",
      "Creating label-encoded income (for tree-based models)...\n",
      "  Income encoding: {'Low income': 0, 'Lower middle income': 1, 'Upper middle income': 2, 'High income': 3}\n",
      "\n",
      "Creating one-hot encoded region with clean names...\n",
      "  Created 6 region dummy variables\n",
      "  Reference category: region_mena_afpak (dropped)\n",
      "  Region columns: ['region_eap', 'region_eca', 'region_lac', 'region_namerica', 'region_sasia', 'region_ssa']\n",
      "\n",
      "Creating one-hot encoded income with clean names (for linear models)...\n",
      "  Created 3 income dummy variables\n",
      "  Reference category: income_low (dropped)\n",
      "  Income columns: ['income_high', 'income_lower_mid', 'income_upper_mid']\n",
      "\n",
      "✓ Categorical features added successfully!\n",
      "  Train shape: (3515, 35)\n",
      "  Val shape: (370, 35)\n",
      "  Test shape: (551, 35)\n"
     ]
    }
   ],
   "source": [
    "# Filter out 'Not classified' income observations\n",
    "print(\"Filtering out 'Not classified' income observations...\")\n",
    "initial_train = len(train_df)\n",
    "initial_val = len(val_df)\n",
    "initial_test = len(test_df)\n",
    "\n",
    "train_df = train_df[train_df['income_level'] != 'Not classified'].copy()\n",
    "val_df = val_df[val_df['income_level'] != 'Not classified'].copy()\n",
    "test_df = test_df[test_df['income_level'] != 'Not classified'].copy()\n",
    "\n",
    "print(f\"  Train: {initial_train} → {len(train_df)} (-{initial_train - len(train_df)})\")\n",
    "print(f\"  Val: {initial_val} → {len(val_df)} (-{initial_val - len(val_df)})\")\n",
    "print(f\"  Test: {initial_test} → {len(test_df)} (-{initial_test - len(test_df)})\")\n",
    "\n",
    "# Create label-encoded income for tree-based models\n",
    "print(\"\\nCreating label-encoded income (for tree-based models)...\")\n",
    "income_mapping = {\n",
    "    'Low income': 0,\n",
    "    'Lower middle income': 1,\n",
    "    'Upper middle income': 2,\n",
    "    'High income': 3\n",
    "}\n",
    "\n",
    "train_df['income_level_encoded'] = train_df['income_level'].map(income_mapping)\n",
    "val_df['income_level_encoded'] = val_df['income_level'].map(income_mapping)\n",
    "test_df['income_level_encoded'] = test_df['income_level'].map(income_mapping)\n",
    "\n",
    "print(f\"  Income encoding: {income_mapping}\")\n",
    "\n",
    "# Create one-hot encoded region with clean names\n",
    "print(\"\\nCreating one-hot encoded region with clean names...\")\n",
    "\n",
    "# Region name mapping to clean snake_case abbreviations\n",
    "region_name_mapping = {\n",
    "    'East Asia & Pacific': 'region_eap',\n",
    "    'Europe & Central Asia': 'region_eca',\n",
    "    'Latin America & Caribbean': 'region_lac',\n",
    "    'Middle East, North Africa, Afghanistan & Pakistan': 'region_mena_afpak',\n",
    "    'North America': 'region_namerica',\n",
    "    'South Asia': 'region_sasia',\n",
    "    'Sub-Saharan Africa': 'region_ssa'\n",
    "}\n",
    "\n",
    "# Create dummies for ALL regions (drop_first=False to get all categories)\n",
    "region_dummies_train = pd.get_dummies(train_df['region'], prefix='region', drop_first=False)\n",
    "region_dummies_val = pd.get_dummies(val_df['region'], prefix='region', drop_first=False)\n",
    "region_dummies_test = pd.get_dummies(test_df['region'], prefix='region', drop_first=False)\n",
    "\n",
    "# Rename columns using mapping\n",
    "for original, clean in region_name_mapping.items():\n",
    "    old_col = f'region_{original}'\n",
    "    if old_col in region_dummies_train.columns:\n",
    "        region_dummies_train.rename(columns={old_col: clean}, inplace=True)\n",
    "    if old_col in region_dummies_val.columns:\n",
    "        region_dummies_val.rename(columns={old_col: clean}, inplace=True)\n",
    "    if old_col in region_dummies_test.columns:\n",
    "        region_dummies_test.rename(columns={old_col: clean}, inplace=True)\n",
    "\n",
    "# Drop reference category: region_mena_afpak\n",
    "reference_region = 'region_mena_afpak'\n",
    "if reference_region in region_dummies_train.columns:\n",
    "    region_dummies_train.drop(columns=[reference_region], inplace=True)\n",
    "if reference_region in region_dummies_val.columns:\n",
    "    region_dummies_val.drop(columns=[reference_region], inplace=True)\n",
    "if reference_region in region_dummies_test.columns:\n",
    "    region_dummies_test.drop(columns=[reference_region], inplace=True)\n",
    "\n",
    "# Ensure all sets have the same columns (in case some regions are missing in val/test)\n",
    "all_region_cols = region_dummies_train.columns.tolist()\n",
    "for col in all_region_cols:\n",
    "    if col not in region_dummies_val.columns:\n",
    "        region_dummies_val[col] = 0\n",
    "    if col not in region_dummies_test.columns:\n",
    "        region_dummies_test[col] = 0\n",
    "\n",
    "# Reorder columns to match\n",
    "region_dummies_val = region_dummies_val[all_region_cols]\n",
    "region_dummies_test = region_dummies_test[all_region_cols]\n",
    "\n",
    "# Add to dataframes\n",
    "train_df = pd.concat([train_df, region_dummies_train], axis=1)\n",
    "val_df = pd.concat([val_df, region_dummies_val], axis=1)\n",
    "test_df = pd.concat([test_df, region_dummies_test], axis=1)\n",
    "\n",
    "print(f\"  Created {len(all_region_cols)} region dummy variables\")\n",
    "print(f\"  Reference category: {reference_region} (dropped)\")\n",
    "print(f\"  Region columns: {all_region_cols}\")\n",
    "\n",
    "# Create one-hot encoded income with clean names (for linear models)\n",
    "print(\"\\nCreating one-hot encoded income with clean names (for linear models)...\")\n",
    "\n",
    "# Income level name mapping to clean snake_case abbreviations\n",
    "income_name_mapping = {\n",
    "    'High income': 'income_high',\n",
    "    'Low income': 'income_low',\n",
    "    'Lower middle income': 'income_lower_mid',\n",
    "    'Upper middle income': 'income_upper_mid'\n",
    "}\n",
    "\n",
    "# Create dummies for ALL income levels (drop_first=False to get all categories)\n",
    "income_dummies_train = pd.get_dummies(train_df['income_level'], prefix='income', drop_first=False)\n",
    "income_dummies_val = pd.get_dummies(val_df['income_level'], prefix='income', drop_first=False)\n",
    "income_dummies_test = pd.get_dummies(test_df['income_level'], prefix='income', drop_first=False)\n",
    "\n",
    "# Rename columns using mapping\n",
    "for original, clean in income_name_mapping.items():\n",
    "    old_col = f'income_{original}'\n",
    "    if old_col in income_dummies_train.columns:\n",
    "        income_dummies_train.rename(columns={old_col: clean}, inplace=True)\n",
    "    if old_col in income_dummies_val.columns:\n",
    "        income_dummies_val.rename(columns={old_col: clean}, inplace=True)\n",
    "    if old_col in income_dummies_test.columns:\n",
    "        income_dummies_test.rename(columns={old_col: clean}, inplace=True)\n",
    "\n",
    "# Drop reference category: income_low\n",
    "reference_income = 'income_low'\n",
    "if reference_income in income_dummies_train.columns:\n",
    "    income_dummies_train.drop(columns=[reference_income], inplace=True)\n",
    "if reference_income in income_dummies_val.columns:\n",
    "    income_dummies_val.drop(columns=[reference_income], inplace=True)\n",
    "if reference_income in income_dummies_test.columns:\n",
    "    income_dummies_test.drop(columns=[reference_income], inplace=True)\n",
    "\n",
    "# Ensure all sets have the same columns\n",
    "all_income_cols = income_dummies_train.columns.tolist()\n",
    "for col in all_income_cols:\n",
    "    if col not in income_dummies_val.columns:\n",
    "        income_dummies_val[col] = 0\n",
    "    if col not in income_dummies_test.columns:\n",
    "        income_dummies_test[col] = 0\n",
    "\n",
    "# Reorder columns to match\n",
    "income_dummies_val = income_dummies_val[all_income_cols]\n",
    "income_dummies_test = income_dummies_test[all_income_cols]\n",
    "\n",
    "# Add to dataframes\n",
    "train_df = pd.concat([train_df, income_dummies_train], axis=1)\n",
    "val_df = pd.concat([val_df, income_dummies_val], axis=1)\n",
    "test_df = pd.concat([test_df, income_dummies_test], axis=1)\n",
    "\n",
    "print(f\"  Created {len(all_income_cols)} income dummy variables\")\n",
    "print(f\"  Reference category: {reference_income} (dropped)\")\n",
    "print(f\"  Income columns: {all_income_cols}\")\n",
    "\n",
    "print(\"\\n✓ Categorical features added successfully!\")\n",
    "print(f\"  Train shape: {train_df.shape}\")\n",
    "print(f\"  Val shape: {val_df.shape}\")\n",
    "print(f\"  Test shape: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db53ea",
   "metadata": {},
   "source": [
    "### Preprocess the data\n",
    "We need three datasets for different model types: raw data for tree-based models for XGBoost and LightGBM, imputed but unscaled for random forest, and imputed and scaled for linear models, SVM and KNN. \n",
    "\n",
    "For imputation, we will take the country median as the default strategy but if the country has all missing values for a feature, we will use the global mean instead.\n",
    "\n",
    "For scaling, we will use standard scaling (mean=0, std=1) based on the training set statistics.\n",
    "\n",
    "Note that we are conducting the imputation and scaling after splitting to avoid data leakage (in other words, using information from the test set to inform the training set transformations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "932682e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor Column Setup:\n",
      "  Base numeric features: 13\n",
      "  Region dummies: 6\n",
      "  Income dummies: 3\n",
      "  Total for tree models: 20 (base + income_encoded + regions)\n",
      "  Total for linear models: 22 (base + income_dummies + regions)\n",
      "\n",
      "Applying log transformation to population_total (before extraction)...\n",
      "  ✓ Log transformation applied to raw data\n",
      "\n",
      "Dataset 1 - Truly Raw/Unimputed (XGBoost, LightGBM):\n",
      "  Using predictor_cols_tree (20 features)\n",
      "  X_train_raw: (3515, 20)\n",
      "  X_val_raw: (370, 20)\n",
      "  X_test_raw: (551, 20)\n",
      "  Missing values - Train: 2103\n",
      "  Missing values - Val: 189\n",
      "  Missing values - Test: 308\n",
      "\n",
      "Imputation complete:\n",
      "Dataset 2 - Imputed (Random Forest):\n",
      "  Using predictor_cols_tree (20 features)\n",
      "  X_train_imputed: (3515, 20)\n",
      "  X_val_imputed: (370, 20)\n",
      "  X_test_imputed: (551, 20)\n",
      "  Missing values - Train: 0\n",
      "  Missing values - Val: 0\n",
      "  Missing values - Test: 0\n",
      "\n",
      "Creating scaled datasets...\n",
      "\n",
      "Dataset 3 - Scaled + Imputed (Linear models, SVM, KNN):\n",
      "  Using predictor_cols_linear (22 features)\n",
      "  X_train_scaled: (3515, 22)\n",
      "  X_val_scaled: (370, 22)\n",
      "  X_test_scaled: (551, 22)\n",
      "\n",
      "Target variable (same for all models):\n",
      "  y_train: (3515,)\n",
      "  y_val: (370,)\n",
      "  y_test: (551,)\n",
      "\n",
      "Setting up time-based cross-validation within the training period...\n",
      "  Using TimeSeriesSplit with 5 splits ordered by year\n",
      "  This keeps validation folds later in time than their training folds\n",
      "\n",
      "✓ All datasets ready for modeling!\n",
      "Tree models: 20 features (numeric + income_encoded + region_dummies)\n",
      "Linear models: 22 features (numeric + income_dummies + region_dummies)\n",
      "Total observations: 4,436\n"
     ]
    }
   ],
   "source": [
    "# Define base predictor columns (numeric features)\n",
    "# Note: Removed unemployment_total, unemployment_female, and labor_force_total\n",
    "# to avoid data leakage (these are mathematically related to FLFP)\n",
    "base_predictor_cols = [\n",
    "    'fertility_rate', 'fertility_adolescent', 'urban_population',\n",
    "    'dependency_ratio', 'life_exp_female', 'infant_mortality',\n",
    "    'population_total', 'secondary_enroll_fe', 'gdp_per_capita_const',\n",
    "    'gdp_growth', 'services_gdp', 'industry_gdp', 'rule_of_law'\n",
    " ]\n",
    "\n",
    "# Get the categorical feature column names that were created in the previous cell\n",
    "region_cols = [col for col in train_df.columns if col.startswith('region_')]\n",
    "# Get income dummy columns (exclude 'income_level' and 'income_level_encoded')\n",
    "income_dummy_cols = [col for col in train_df.columns if col.startswith('income_') and col not in ['income_level', 'income_level_encoded']]\n",
    "\n",
    "# Create separate predictor lists for different model types\n",
    "# Tree-based models: use label-encoded income + one-hot region\n",
    "predictor_cols_tree = base_predictor_cols + ['income_level_encoded'] + region_cols\n",
    "\n",
    "# Linear models: use one-hot encoded income + one-hot region\n",
    "predictor_cols_linear = base_predictor_cols + income_dummy_cols + region_cols\n",
    "\n",
    "print(\"Predictor Column Setup:\")\n",
    "print(f\"  Base numeric features: {len(base_predictor_cols)}\")\n",
    "print(f\"  Region dummies: {len(region_cols)}\")\n",
    "print(f\"  Income dummies: {len(income_dummy_cols)}\")\n",
    "print(f\"  Total for tree models: {len(predictor_cols_tree)} (base + income_encoded + regions)\")\n",
    "print(f\"  Total for linear models: {len(predictor_cols_linear)} (base + income_dummies + regions)\")\n",
    "\n",
    "target_col = 'flfp_15_64'\n",
    "\n",
    "# Variables that need imputation (only numeric features need imputation)\n",
    "variables_to_impute = [\n",
    "    'secondary_enroll_fe', 'urban_population', 'infant_mortality',\n",
    "    'gdp_per_capita_const', 'gdp_growth', 'services_gdp',\n",
    "    'industry_gdp', 'rule_of_law'\n",
    " ]\n",
    "\n",
    "def panel_imputation(train_df, val_df, test_df, variables_to_impute):\n",
    "    \"\"\"Apply country-specific median imputation without data leakage\"\"\"\n",
    "\n",
    "    # Calculate imputation rules using ONLY training data\n",
    "    train_country_medians = {}\n",
    "    train_year_medians = {}\n",
    "    train_global_medians = {}\n",
    "\n",
    "    for var in variables_to_impute:\n",
    "        # Country-specific medians from training data only\n",
    "        train_country_medians[var] = train_df.groupby('country_name')[var].median()\n",
    "        # Year-specific medians from training data only\n",
    "        train_year_medians[var] = train_df.groupby('year')[var].median()\n",
    "        # Global median from training data only\n",
    "        train_global_medians[var] = train_df[var].median()\n",
    "\n",
    "    # Apply imputation rules to all datasets\n",
    "    def apply_imputation(df):\n",
    "        df_imputed = df.copy()\n",
    "        for var in variables_to_impute:\n",
    "            if var in df_imputed.columns:\n",
    "                # Use training-based country medians\n",
    "                for country in df_imputed['country_name'].unique():\n",
    "                    country_mask = df_imputed['country_name'] == country\n",
    "                    country_median = train_country_medians[var].get(country, np.nan)\n",
    "\n",
    "                    # Fill using country median where available\n",
    "                    df_imputed.loc[country_mask, var] = df_imputed.loc[country_mask, var].fillna(country_median)\n",
    "\n",
    "                # Fill remaining NaNs using year medians\n",
    "                year_values = df_imputed.loc[:, 'year']\n",
    "                missing_mask = df_imputed[var].isna()\n",
    "                if missing_mask.any():\n",
    "                    years_to_fill = year_values[missing_mask]\n",
    "                    fill_values = years_to_fill.map(train_year_medians[var]).astype(float)\n",
    "                    df_imputed.loc[missing_mask, var] = fill_values\n",
    "\n",
    "                # Fall back to training-global median if still missing\n",
    "                df_imputed[var] = df_imputed[var].fillna(train_global_medians[var])\n",
    "\n",
    "        return df_imputed\n",
    "\n",
    "    return apply_imputation(train_df), apply_imputation(val_df), apply_imputation(test_df)\n",
    "\n",
    "# Log-transform population_total in the original dataframes (before imputation)\n",
    "# This will affect both raw and imputed datasets\n",
    "print(\"\\nApplying log transformation to population_total (before extraction)...\")\n",
    "train_df['population_total'] = np.log(train_df['population_total'])\n",
    "val_df['population_total'] = np.log(val_df['population_total'])\n",
    "test_df['population_total'] = np.log(test_df['population_total'])\n",
    "print(\"  ✓ Log transformation applied to raw data\")\n",
    "\n",
    "# Extract TRULY raw features (before imputation) for models that handle missing values natively\n",
    "# Use tree predictor columns (includes income_level_encoded + region dummies)\n",
    "X_train_raw = train_df[predictor_cols_tree].copy()\n",
    "X_val_raw = val_df[predictor_cols_tree].copy()\n",
    "X_test_raw = test_df[predictor_cols_tree].copy()\n",
    "\n",
    "print(\"\\nDataset 1 - Truly Raw/Unimputed (XGBoost, LightGBM):\")\n",
    "print(f\"  Using predictor_cols_tree ({len(predictor_cols_tree)} features)\")\n",
    "print(f\"  X_train_raw: {X_train_raw.shape}\")\n",
    "print(f\"  X_val_raw: {X_val_raw.shape}\")\n",
    "print(f\"  X_test_raw: {X_test_raw.shape}\")\n",
    "print(f\"  Missing values - Train: {X_train_raw.isna().sum().sum()}\")\n",
    "print(f\"  Missing values - Val: {X_val_raw.isna().sum().sum()}\")\n",
    "print(f\"  Missing values - Test: {X_test_raw.isna().sum().sum()}\")\n",
    "\n",
    "# Apply imputation\n",
    "train_clean, val_clean, test_clean = panel_imputation(\n",
    "    train_df, val_df, test_df, variables_to_impute\n",
    ")\n",
    "\n",
    "# Extract imputed features and target\n",
    "# Use tree predictor columns (includes income_level_encoded + region dummies)\n",
    "X_train_imputed = train_clean[predictor_cols_tree].copy()\n",
    "X_val_imputed = val_clean[predictor_cols_tree].copy()\n",
    "X_test_imputed = test_clean[predictor_cols_tree].copy()\n",
    "\n",
    "y_train = train_clean[target_col].copy()\n",
    "y_val = val_clean[target_col].copy()\n",
    "y_test = test_clean[target_col].copy()\n",
    "\n",
    "print(\"\\nImputation complete:\")\n",
    "print(\"Dataset 2 - Imputed (Random Forest):\")\n",
    "print(f\"  Using predictor_cols_tree ({len(predictor_cols_tree)} features)\")\n",
    "print(f\"  X_train_imputed: {X_train_imputed.shape}\")\n",
    "print(f\"  X_val_imputed: {X_val_imputed.shape}\")\n",
    "print(f\"  X_test_imputed: {X_test_imputed.shape}\")\n",
    "print(f\"  Missing values - Train: {X_train_imputed.isna().sum().sum()}\")\n",
    "print(f\"  Missing values - Val: {X_val_imputed.isna().sum().sum()}\")\n",
    "print(f\"  Missing values - Test: {X_test_imputed.isna().sum().sum()}\")\n",
    "\n",
    "# Create scaled versions (fit scaler only on training data)\n",
    "# Use linear predictor columns (includes income dummies + region dummies)\n",
    "print(\"\\nCreating scaled datasets...\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Need to extract linear predictor columns from the clean dataframes\n",
    "X_train_linear = train_clean[predictor_cols_linear].copy()\n",
    "X_val_linear = val_clean[predictor_cols_linear].copy()\n",
    "X_test_linear = test_clean[predictor_cols_linear].copy()\n",
    "\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_linear),\n",
    "    columns=predictor_cols_linear,\n",
    "    index=X_train_linear.index\n",
    ")\n",
    "X_val_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_val_linear),\n",
    "    columns=predictor_cols_linear,\n",
    "    index=X_val_linear.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test_linear),\n",
    "    columns=predictor_cols_linear,\n",
    "    index=X_test_linear.index\n",
    ")\n",
    "\n",
    "print(\"\\nDataset 3 - Scaled + Imputed (Linear models, SVM, KNN):\")\n",
    "print(f\"  Using predictor_cols_linear ({len(predictor_cols_linear)} features)\")\n",
    "print(f\"  X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"  X_val_scaled: {X_val_scaled.shape}\")\n",
    "print(f\"  X_test_scaled: {X_test_scaled.shape}\")\n",
    "\n",
    "print(\"\\nTarget variable (same for all models):\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_val: {y_val.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")\n",
    "\n",
    "# Create time-based cross-validation within the training period\n",
    "print(\"\\nSetting up time-based cross-validation within the training period...\")\n",
    "\n",
    "# Sort training data by year so TimeSeriesSplit respects chronology\n",
    "train_clean_sorted = train_clean.sort_values('year')\n",
    "\n",
    "# Reorder feature matrices and target to match this sorted index\n",
    "X_train_raw = X_train_raw.loc[train_clean_sorted.index]\n",
    "X_train_imputed = X_train_imputed.loc[train_clean_sorted.index]\n",
    "X_train_scaled = X_train_scaled.loc[train_clean_sorted.index]\n",
    "y_train = y_train.loc[train_clean_sorted.index]\n",
    "\n",
    "# Time-based CV: each split trains on earlier years and validates on later years\n",
    "time_kfold = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print(f\"  Using TimeSeriesSplit with {time_kfold.n_splits} splits ordered by year\")\n",
    "print(\"  This keeps validation folds later in time than their training folds\")\n",
    "\n",
    "print(\"\\n✓ All datasets ready for modeling!\")\n",
    "print(f\"Tree models: {len(predictor_cols_tree)} features (numeric + income_encoded + region_dummies)\")\n",
    "print(f\"Linear models: {len(predictor_cols_linear)} features (numeric + income_dummies + region_dummies)\")\n",
    "print(f\"Total observations: {len(X_train_raw) + len(X_val_raw) + len(X_test_raw):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1633b80",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef65c12",
   "metadata": {},
   "source": [
    "### Simple OLS (no regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dc83b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Performance:\n",
      "  RMSE: 11.022\n",
      "  MAE:  8.460\n",
      "  R²:   0.509\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: 11.706\n",
      "  MAE:  8.969\n",
      "  R²:   0.397\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                 feature  importance\n",
      "21            region_ssa   11.945144\n",
      "17            region_eca   11.345783\n",
      "16            region_eap   11.121724\n",
      "18            region_lac    9.844976\n",
      "3       dependency_ratio    7.796105\n",
      "4        life_exp_female    6.142638\n",
      "15      income_upper_mid    4.647287\n",
      "13           income_high    4.312782\n",
      "8   gdp_per_capita_const    3.681507\n",
      "0         fertility_rate    3.102939\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "ols_model = LinearRegression()\n",
    "\n",
    "# Fit on training data (using scaled data for linear models)\n",
    "ols_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on train and validation sets\n",
    "y_train_pred = ols_model.predict(X_train_scaled)\n",
    "y_val_pred = ols_model.predict(X_val_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nTraining Performance:\")\n",
    "print(f\"  RMSE: {train_rmse:.3f}\")\n",
    "print(f\"  MAE:  {train_mae:.3f}\")\n",
    "print(f\"  R²:   {train_r2:.3f}\")\n",
    "\n",
    "print(f\"\\nValidation Performance:\")\n",
    "print(f\"  RMSE: {val_rmse:.3f}\")\n",
    "print(f\"  MAE:  {val_mae:.3f}\")\n",
    "print(f\"  R²:   {val_r2:.3f}\")\n",
    "\n",
    "# Feature importance for OLS (coefficient magnitudes)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': predictor_cols_linear,\n",
    "    'importance': np.abs(ols_model.coef_)\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0290840",
   "metadata": {},
   "source": [
    "### Lasso Regression (L1 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fc11b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha (regularization strength): 0.01\n",
      "Cross-validation score: 123.082\n",
      "\n",
      "Training Performance:\n",
      "  RMSE: 11.023\n",
      "  MAE:  8.468\n",
      "  R²:   0.509\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: 11.685\n",
      "  MAE:  8.948\n",
      "  R²:   0.399\n",
      "\n",
      "Feature Selection:\n",
      "  Features selected: 22/22\n",
      "  Features eliminated: 0\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                feature  importance\n",
      "0        fertility_rate    2.807001\n",
      "1  fertility_adolescent    2.298461\n",
      "2      urban_population    1.409252\n",
      "3      dependency_ratio    7.509818\n",
      "4       life_exp_female    5.919945\n",
      "5      infant_mortality    2.822093\n",
      "6      population_total    0.635869\n",
      "7   secondary_enroll_fe    0.141356\n",
      "8  gdp_per_capita_const    3.652596\n",
      "9            gdp_growth    0.289871\n"
     ]
    }
   ],
   "source": [
    "# Define alpha values to test (regularization strength)\n",
    "alpha_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "# Initialize Lasso with GridSearch for hyperparameter tuning\n",
    "lasso_grid = GridSearchCV(\n",
    "    Lasso(random_state=42, max_iter=2000),\n",
    "    param_grid={'alpha': alpha_values},\n",
    "    cv=time_kfold,  # Time-based CV within training period\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on training data (using scaled data)\n",
    "lasso_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model\n",
    "lasso_model = lasso_grid.best_estimator_\n",
    "best_alpha = lasso_grid.best_params_['alpha']\n",
    "\n",
    "print(f\"Best alpha (regularization strength): {best_alpha}\")\n",
    "print(f\"Cross-validation score: {-lasso_grid.best_score_:.3f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = lasso_model.predict(X_train_scaled)\n",
    "y_val_pred = lasso_model.predict(X_val_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Display results (focus on fit measures)\n",
    "print(f\"\\nTraining Performance:\")\n",
    "print(f\"  RMSE: {train_rmse:.3f}\")\n",
    "print(f\"  MAE:  {train_mae:.3f}\")\n",
    "print(f\"  R²:   {train_r2:.3f}\")\n",
    "\n",
    "print(f\"\\nValidation Performance:\")\n",
    "print(f\"  RMSE: {val_rmse:.3f}\")\n",
    "print(f\"  MAE:  {val_mae:.3f}\")\n",
    "print(f\"  R²:   {val_r2:.3f}\")\n",
    "\n",
    "# Show feature selection results\n",
    "non_zero_features = np.sum(lasso_model.coef_ != 0)\n",
    "print(f\"\\nFeature Selection:\")\n",
    "print(f\"  Features selected: {non_zero_features}/{len(predictor_cols_linear)}\")\n",
    "print(f\"  Features eliminated: {len(predictor_cols_linear) - non_zero_features}\")\n",
    "\n",
    "# Coefficient magnitudes (features are scaled)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': predictor_cols_linear,\n",
    "    'importance': np.abs(lasso_model.coef_)\n",
    "})\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e90b4",
   "metadata": {},
   "source": [
    "### Ridge Regression (L2 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cb9cea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha (regularization strength): 1.0\n",
      "Cross-validation score: 123.182\n",
      "\n",
      "Training Performance:\n",
      "  RMSE: 11.022\n",
      "  MAE:  8.462\n",
      "  R²:   0.509\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: 11.703\n",
      "  MAE:  8.969\n",
      "  R²:   0.397\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                 feature  importance\n",
      "21            region_ssa   11.915953\n",
      "17            region_eca   11.300181\n",
      "16            region_eap   11.091472\n",
      "18            region_lac    9.805426\n",
      "3       dependency_ratio    7.749371\n",
      "4        life_exp_female    6.120997\n",
      "15      income_upper_mid    4.618732\n",
      "13           income_high    4.276166\n",
      "8   gdp_per_capita_const    3.683888\n",
      "0         fertility_rate    3.045783\n"
     ]
    }
   ],
   "source": [
    "# Define alpha values to test (regularization strength)\n",
    "alpha_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "# Initialize Ridge with GridSearch for hyperparameter tuning\n",
    "ridge_grid = GridSearchCV(\n",
    "    Ridge(random_state=42),\n",
    "    param_grid={'alpha': alpha_values},\n",
    "    cv=time_kfold,  # Time-based CV within training period\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on training data (using scaled data)\n",
    "ridge_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model\n",
    "ridge_model = ridge_grid.best_estimator_\n",
    "best_alpha = ridge_grid.best_params_['alpha']\n",
    "\n",
    "print(f\"Best alpha (regularization strength): {best_alpha}\")\n",
    "print(f\"Cross-validation score: {-ridge_grid.best_score_:.3f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = ridge_model.predict(X_train_scaled)\n",
    "y_val_pred = ridge_model.predict(X_val_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nTraining Performance:\")\n",
    "print(f\"  RMSE: {train_rmse:.3f}\")\n",
    "print(f\"  MAE:  {train_mae:.3f}\")\n",
    "print(f\"  R²:   {train_r2:.3f}\")\n",
    "\n",
    "print(f\"\\nValidation Performance:\")\n",
    "print(f\"  RMSE: {val_rmse:.3f}\")\n",
    "print(f\"  MAE:  {val_mae:.3f}\")\n",
    "print(f\"  R²:   {val_r2:.3f}\")\n",
    "\n",
    "# Feature importance for Ridge (coefficient magnitudes)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': predictor_cols_linear,\n",
    "    'importance': np.abs(ridge_model.coef_)\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0014ce56",
   "metadata": {},
   "source": [
    "### Elastic Net Regression (L1+L2 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f43b07fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Elastic Net with hyperparameter tuning...\n",
      "Best hyperparameters:\n",
      "  alpha: 0.01\n",
      "  l1_ratio: 0.5\n",
      "Cross-validation score: 122.971\n",
      "\n",
      "Training Performance:\n",
      "  RMSE: 11.034\n",
      "  MAE:  8.508\n",
      "  R²:   0.508\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: 11.662\n",
      "  MAE:  8.958\n",
      "  R²:   0.402\n",
      "\n",
      "Top 10 features by |coefficient|:\n",
      "                 feature       coef   abs_coef\n",
      "21            region_ssa  11.441329  11.441329\n",
      "16            region_eap  10.580470  10.580470\n",
      "17            region_eca  10.534444  10.534444\n",
      "18            region_lac   9.139997   9.139997\n",
      "3       dependency_ratio  -6.937745   6.937745\n",
      "4        life_exp_female  -5.694028   5.694028\n",
      "15      income_upper_mid  -4.116431   4.116431\n",
      "8   gdp_per_capita_const   3.695069   3.695069\n",
      "13           income_high  -3.628104   3.628104\n",
      "19       region_namerica   2.602425   2.602425\n"
     ]
    }
   ],
   "source": [
    "# Define grids for Elastic Net\n",
    "alpha_values = [0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "l1_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]  # 0 → pure Ridge, 1 → pure Lasso\n",
    "\n",
    "elastic_grid = GridSearchCV(\n",
    "    ElasticNet(max_iter=5000, random_state=42),\n",
    "    param_grid={'alpha': alpha_values, 'l1_ratio': l1_ratios},\n",
    "    cv=time_kfold,                 # time-based CV within training period\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Fitting Elastic Net with hyperparameter tuning...\")\n",
    "elastic_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "elastic_model = elastic_grid.best_estimator_\n",
    "best_params = elastic_grid.best_params_\n",
    "\n",
    "print(f\"Best hyperparameters:\")\n",
    "print(f\"  alpha: {best_params['alpha']}\")\n",
    "print(f\"  l1_ratio: {best_params['l1_ratio']}\")\n",
    "print(f\"Cross-validation score: {-elastic_grid.best_score_:.3f}\")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = elastic_model.predict(X_train_scaled)\n",
    "y_val_pred = elastic_model.predict(X_val_scaled)\n",
    "\n",
    "# Metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"\\nTraining Performance:\")\n",
    "print(f\"  RMSE: {train_rmse:.3f}\")\n",
    "print(f\"  MAE:  {train_mae:.3f}\")\n",
    "print(f\"  R²:   {train_r2:.3f}\")\n",
    "\n",
    "print(f\"\\nValidation Performance:\")\n",
    "print(f\"  RMSE: {val_rmse:.3f}\")\n",
    "print(f\"  MAE:  {val_mae:.3f}\")\n",
    "print(f\"  R²:   {val_r2:.3f}\")\n",
    "\n",
    "# Coefficient magnitudes\n",
    "coef_importance = pd.DataFrame({\n",
    "    'feature': predictor_cols_linear,\n",
    "    'coef': elastic_model.coef_,\n",
    "    'abs_coef': np.abs(elastic_model.coef_)\n",
    "}).sort_values('abs_coef', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features by |coefficient|:\")\n",
    "print(coef_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a010fb1",
   "metadata": {},
   "source": [
    "## Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55bc9619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SVR (RBF) with more regularized hyperparameter grid...\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Best hyperparameters:\n",
      "  C (regularization): 10\n",
      "  Gamma (kernel coef): auto\n",
      "  Epsilon (tolerance): 0.5\n",
      "Cross-validation score: 56.455\n",
      "\n",
      "Training Performance:\n",
      "  RMSE: 5.958\n",
      "  MAE:  3.489\n",
      "  R²:   0.857\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: 7.780\n",
      "  MAE:  5.393\n",
      "  R²:   0.734\n",
      "\n",
      "Model Characteristics:\n",
      "  Kernel: RBF (Radial Basis Function)\n",
      "  Support vectors: [2958]\n",
      "  Non-linear decision boundary\n"
     ]
    }
   ],
   "source": [
    "# Set simpler, regularized grid\n",
    "param_grid = {\n",
    "    # slightly favor smaller C (stronger regularization)\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    # a bit wider epsilon range (flatter function, less overfit)\n",
    "    'epsilon': [0.05, 0.1, 0.2, 0.5],\n",
    "    # avoid very small fixed gamma that can lead to overfitting\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "svr_grid = GridSearchCV(\n",
    "    SVR(kernel='rbf'),\n",
    "    param_grid=param_grid,\n",
    "    cv=time_kfold,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Fitting SVR (RBF) with more regularized hyperparameter grid...\")\n",
    "svr_grid.fit(X_train_scaled, y_train)\n",
    "svr_model = svr_grid.best_estimator_\n",
    "best_params = svr_grid.best_params_\n",
    "\n",
    "print(f\"Best hyperparameters:\")\n",
    "print(f\"  C (regularization): {best_params['C']}\")\n",
    "print(f\"  Gamma (kernel coef): {best_params['gamma']}\")\n",
    "print(f\"  Epsilon (tolerance): {best_params['epsilon']}\")\n",
    "print(f\"Cross-validation score: {-svr_grid.best_score_:.3f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = svr_model.predict(X_train_scaled)\n",
    "y_val_pred = svr_model.predict(X_val_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nTraining Performance:\")\n",
    "print(f\"  RMSE: {train_rmse:.3f}\")\n",
    "print(f\"  MAE:  {train_mae:.3f}\")\n",
    "print(f\"  R²:   {train_r2:.3f}\")\n",
    "\n",
    "print(f\"\\nValidation Performance:\")\n",
    "print(f\"  RMSE: {val_rmse:.3f}\")\n",
    "print(f\"  MAE:  {val_mae:.3f}\")\n",
    "print(f\"  R²:   {val_r2:.3f}\")\n",
    "\n",
    "print(f\"\\nModel Characteristics:\")\n",
    "print(f\"  Kernel: RBF (Radial Basis Function)\")\n",
    "print(f\"  Support vectors: {svr_model.n_support_}\")\n",
    "print(f\"  Non-linear decision boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bcb27b",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors Regression (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cff9f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting KNN with hyperparameter tuning...\n",
      "Best hyperparameters:\n",
      "  n_neighbors: 3\n",
      "  weights: distance\n",
      "  metric: manhattan\n",
      "Cross-validation score: 8.704\n",
      "\n",
      "Training Performance:\n",
      "  RMSE: 0.000\n",
      "  MAE:  0.000\n",
      "  R²:   1.000\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: 2.415\n",
      "  MAE:  1.544\n",
      "  R²:   0.974\n",
      "\n",
      "Model Characteristics:\n",
      "  Non-parametric model\n",
      "  Memory-based learning\n",
      "  Local predictions based on nearest neighbors\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters to test\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 10, 15, 20],     # Number of neighbors\n",
    "    'weights': ['uniform', 'distance'],        # Weighting scheme\n",
    "    'metric': ['euclidean', 'manhattan']       # Distance metric\n",
    "}\n",
    "\n",
    "# Initialize KNN with GridSearch for hyperparameter tuning\n",
    "knn_grid = GridSearchCV(\n",
    "    KNeighborsRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    cv=time_kfold,  # Time-based CV within training period\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Fitting KNN with hyperparameter tuning...\")\n",
    "\n",
    "# Fit on training data (using scaled data - KNN needs scaling!)\n",
    "knn_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model\n",
    "knn_model = knn_grid.best_estimator_\n",
    "best_params = knn_grid.best_params_\n",
    "\n",
    "print(f\"Best hyperparameters:\")\n",
    "print(f\"  n_neighbors: {best_params['n_neighbors']}\")\n",
    "print(f\"  weights: {best_params['weights']}\")\n",
    "print(f\"  metric: {best_params['metric']}\")\n",
    "print(f\"Cross-validation score: {-knn_grid.best_score_:.3f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = knn_model.predict(X_train_scaled)\n",
    "y_val_pred = knn_model.predict(X_val_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nTraining Performance:\")\n",
    "print(f\"  RMSE: {train_rmse:.3f}\")\n",
    "print(f\"  MAE:  {train_mae:.3f}\")\n",
    "print(f\"  R²:   {train_r2:.3f}\")\n",
    "\n",
    "print(f\"\\nValidation Performance:\")\n",
    "print(f\"  RMSE: {val_rmse:.3f}\")\n",
    "print(f\"  MAE:  {val_mae:.3f}\")\n",
    "print(f\"  R²:   {val_r2:.3f}\")\n",
    "\n",
    "print(f\"\\nModel Characteristics:\")\n",
    "print(f\"  Non-parametric model\")\n",
    "print(f\"  Memory-based learning\")\n",
    "print(f\"  Local predictions based on nearest neighbors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d0e13",
   "metadata": {},
   "source": [
    "## Tree-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2371344a",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb2db99",
   "metadata": {},
   "source": [
    "**Note**: Using reduced hyperparameter grids for initial model comparison. These smaller grids provide sufficient exploration to compare algorithm performance while keeping runtime manageable. Full hyperparameter optimization can be done later for the best-performing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dce672fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Random Forest with hyperparameter tuning...\n",
      "This may take several minutes...\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best hyperparameters:\n",
      "  n_estimators: 200\n",
      "  max_depth: None\n",
      "  min_samples_split: 2\n",
      "  min_samples_leaf: 1\n",
      "  max_features: 0.3\n",
      "Cross-validation score: 39.647\n",
      "\n",
      "Training Performance:\n",
      "  RMSE: 0.933\n",
      "  MAE:  0.595\n",
      "  R²:   0.996\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: 5.407\n",
      "  MAE:  3.869\n",
      "  R²:   0.871\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                 feature  importance\n",
      "6       population_total    0.105949\n",
      "7    secondary_enroll_fe    0.090172\n",
      "0         fertility_rate    0.086890\n",
      "1   fertility_adolescent    0.079364\n",
      "2       urban_population    0.074516\n",
      "4        life_exp_female    0.070776\n",
      "8   gdp_per_capita_const    0.070467\n",
      "12           rule_of_law    0.066866\n",
      "14            region_eap    0.059568\n",
      "19            region_ssa    0.051988\n"
     ]
    }
   ],
   "source": [
    "# Note that we use the imputed but unscaled data for Random Forest\n",
    "\n",
    "# Define hyperparameters to test (reduced grid for initial comparison)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],               # Number of trees\n",
    "    'max_depth': [8, 12, None],               # Maximum tree depth\n",
    "    'min_samples_split': [2, 5, 10],          # Min samples to split node\n",
    "    'min_samples_leaf': [1, 2],               # Min samples in leaf\n",
    "    'max_features': [0.2, 0.3, 'sqrt']        # Features per split\n",
    "}\n",
    "\n",
    "# Initialize Random Forest with GridSearch for hyperparameter tuning\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    param_grid=param_grid,\n",
    "    cv=time_kfold,  # Time-based CV within training period\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1  # Show progress\n",
    ")\n",
    "\n",
    "print(\"Fitting Random Forest with hyperparameter tuning...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Fit on training data (using imputed but unscaled data)\n",
    "rf_grid.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Get the best model\n",
    "rf_model = rf_grid.best_estimator_\n",
    "best_params = rf_grid.best_params_\n",
    "\n",
    "print(f\"Best hyperparameters:\")\n",
    "print(f\"  n_estimators: {best_params['n_estimators']}\")\n",
    "print(f\"  max_depth: {best_params['max_depth']}\")\n",
    "print(f\"  min_samples_split: {best_params['min_samples_split']}\")\n",
    "print(f\"  min_samples_leaf: {best_params['min_samples_leaf']}\")\n",
    "print(f\"  max_features: {best_params['max_features']}\")\n",
    "print(f\"Cross-validation score: {-rf_grid.best_score_:.3f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = rf_model.predict(X_train_imputed)\n",
    "y_val_pred = rf_model.predict(X_val_imputed)\n",
    "\n",
    "# Calculate performance metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nTraining Performance:\")\n",
    "print(f\"  RMSE: {train_rmse:.3f}\")\n",
    "print(f\"  MAE:  {train_mae:.3f}\")\n",
    "print(f\"  R²:   {train_r2:.3f}\")\n",
    "\n",
    "print(f\"\\nValidation Performance:\")\n",
    "print(f\"  RMSE: {val_rmse:.3f}\")\n",
    "print(f\"  MAE:  {val_mae:.3f}\")\n",
    "print(f\"  R²:   {val_r2:.3f}\")\n",
    "\n",
    "# Save feature importance for model interpretation and comparison\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': predictor_cols_tree,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf36e43",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cf983ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting XGBoost with hyperparameter tuning...\n",
      "This may take several minutes...\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Best hyperparameters:\n",
      "  n_estimators: 200\n",
      "  max_depth: 6\n",
      "  learning_rate: 0.1\n",
      "  subsample: 1.0\n",
      "  colsample_bytree: 0.9\n",
      "Cross-validation score: 44.433\n",
      "\n",
      "Training Performance:\n",
      "  RMSE: 0.923\n",
      "  MAE:  0.643\n",
      "  R²:   0.997\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: 6.104\n",
      "  MAE:  4.073\n",
      "  R²:   0.836\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                 feature  importance\n",
      "19            region_ssa    0.241206\n",
      "14            region_eap    0.191705\n",
      "16            region_lac    0.188869\n",
      "15            region_eca    0.115791\n",
      "13  income_level_encoded    0.059938\n",
      "4        life_exp_female    0.032786\n",
      "6       population_total    0.023580\n",
      "8   gdp_per_capita_const    0.022789\n",
      "2       urban_population    0.015393\n",
      "0         fertility_rate    0.015321\n"
     ]
    }
   ],
   "source": [
    "# Note that we use the raw unimputed data for XGBoost because it can handle missing values natively\n",
    "\n",
    "# Clean column names to snake_case for consistency\n",
    "X_train_raw_xgb = X_train_raw.copy()\n",
    "X_val_raw_xgb = X_val_raw.copy()\n",
    "X_test_raw_xgb = X_test_raw.copy()\n",
    "\n",
    "# Convert to snake_case: lowercase, replace special chars and spaces with underscores\n",
    "X_train_raw_xgb.columns = (X_train_raw_xgb.columns\n",
    "                            .str.lower()\n",
    "                            .str.replace('[^a-z0-9]+', '_', regex=True)\n",
    "                            .str.strip('_'))\n",
    "X_val_raw_xgb.columns = (X_val_raw_xgb.columns\n",
    "                          .str.lower()\n",
    "                          .str.replace('[^a-z0-9]+', '_', regex=True)\n",
    "                          .str.strip('_'))\n",
    "X_test_raw_xgb.columns = (X_test_raw_xgb.columns\n",
    "                           .str.lower()\n",
    "                           .str.replace('[^a-z0-9]+', '_', regex=True)\n",
    "                           .str.strip('_'))\n",
    "\n",
    "# Define hyperparameters to test (reduced grid for initial comparison)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],               # Number of boosting rounds\n",
    "    'max_depth': [3, 6],                      # Maximum tree depth\n",
    "    'learning_rate': [0.01, 0.1],             # Step size shrinkage\n",
    "    'subsample': [0.9, 1.0],                  # Fraction of samples per tree\n",
    "    'colsample_bytree': [0.9, 1.0]            # Fraction of features per tree\n",
    "}\n",
    "\n",
    "# Initialize XGBoost with GridSearch for hyperparameter tuning\n",
    "xgb_grid = GridSearchCV(\n",
    "    xgb.XGBRegressor(random_state=42, n_jobs=-1),\n",
    "    param_grid=param_grid,\n",
    "    cv=time_kfold,  # Time-based CV within training period\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1  # Show progress\n",
    ")\n",
    "\n",
    "print(\"Fitting XGBoost with hyperparameter tuning...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Fit on cleaned training data\n",
    "xgb_grid.fit(X_train_raw_xgb, y_train)\n",
    "\n",
    "# Get the best model\n",
    "xgb_model = xgb_grid.best_estimator_\n",
    "best_params = xgb_grid.best_params_\n",
    "\n",
    "print(f\"Best hyperparameters:\")\n",
    "print(f\"  n_estimators: {best_params['n_estimators']}\")\n",
    "print(f\"  max_depth: {best_params['max_depth']}\")\n",
    "print(f\"  learning_rate: {best_params['learning_rate']}\")\n",
    "print(f\"  subsample: {best_params['subsample']}\")\n",
    "print(f\"  colsample_bytree: {best_params['colsample_bytree']}\")\n",
    "print(f\"Cross-validation score: {-xgb_grid.best_score_:.3f}\")\n",
    "\n",
    "# Make predictions using cleaned data\n",
    "y_train_pred = xgb_model.predict(X_train_raw_xgb)\n",
    "y_val_pred = xgb_model.predict(X_val_raw_xgb)\n",
    "\n",
    "# Calculate performance metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nTraining Performance:\")\n",
    "print(f\"  RMSE: {train_rmse:.3f}\")\n",
    "print(f\"  MAE:  {train_mae:.3f}\")\n",
    "print(f\"  R²:   {train_r2:.3f}\")\n",
    "\n",
    "print(f\"\\nValidation Performance:\")\n",
    "print(f\"  RMSE: {val_rmse:.3f}\")\n",
    "print(f\"  MAE:  {val_mae:.3f}\")\n",
    "print(f\"  R²:   {val_r2:.3f}\")\n",
    "\n",
    "# Feature importance for XGBoost\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': predictor_cols_tree,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c119f6",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5200fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LightGBM with hyperparameter tuning...\n",
      "This may take several minutes...\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "Best hyperparameters:\n",
      "  n_estimators: 200\n",
      "  max_depth: 6\n",
      "  learning_rate: 0.1\n",
      "  subsample: 0.9\n",
      "  colsample_bytree: 0.9\n",
      "  num_leaves: 31\n",
      "Cross-validation score: 40.887\n",
      "\n",
      "Training Performance:\n",
      "  RMSE: 1.382\n",
      "  MAE:  0.970\n",
      "  R²:   0.992\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: 6.481\n",
      "  MAE:  4.147\n",
      "  R²:   0.815\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                 feature  importance\n",
      "6       population_total         581\n",
      "2       urban_population         483\n",
      "8   gdp_per_capita_const         478\n",
      "1   fertility_adolescent         438\n",
      "12           rule_of_law         378\n",
      "10          services_gdp         374\n",
      "7    secondary_enroll_fe         340\n",
      "5       infant_mortality         332\n",
      "0         fertility_rate         302\n",
      "11          industry_gdp         290\n"
     ]
    }
   ],
   "source": [
    "# Note that we use the raw unimputed data for LightGBM because it can handle missing values natively\n",
    "\n",
    "# IMPORTANT: Clean column names for LightGBM (it doesn't support special characters)\n",
    "# Convert to snake_case for consistency\n",
    "X_train_raw_lgb = X_train_raw.copy()\n",
    "X_val_raw_lgb = X_val_raw.copy()\n",
    "X_test_raw_lgb = X_test_raw.copy()\n",
    "\n",
    "# Convert to snake_case: lowercase, replace special chars and spaces with underscores\n",
    "X_train_raw_lgb.columns = (X_train_raw_lgb.columns\n",
    "                            .str.lower()\n",
    "                            .str.replace('[^a-z0-9]+', '_', regex=True)\n",
    "                            .str.strip('_'))\n",
    "X_val_raw_lgb.columns = (X_val_raw_lgb.columns\n",
    "                          .str.lower()\n",
    "                          .str.replace('[^a-z0-9]+', '_', regex=True)\n",
    "                          .str.strip('_'))\n",
    "X_test_raw_lgb.columns = (X_test_raw_lgb.columns\n",
    "                           .str.lower()\n",
    "                           .str.replace('[^a-z0-9]+', '_', regex=True)\n",
    "                           .str.strip('_'))\n",
    "\n",
    "# Define hyperparameters to test (reduced grid for initial comparison)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],               # Number of boosting rounds\n",
    "    'max_depth': [3, 6],                      # Maximum tree depth\n",
    "    'learning_rate': [0.01, 0.1],             # Step size shrinkage\n",
    "    'subsample': [0.9, 1.0],                  # Fraction of samples per tree\n",
    "    'colsample_bytree': [0.9, 1.0],           # Fraction of features per tree\n",
    "    'num_leaves': [31, 50]                    # Maximum number of leaves per tree\n",
    "}\n",
    "\n",
    "# Initialize LightGBM with GridSearch for hyperparameter tuning\n",
    "lgb_grid = GridSearchCV(\n",
    "    lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),\n",
    "    param_grid=param_grid,\n",
    "    cv=time_kfold,  # Time-based CV within training period\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1  # Show progress\n",
    ")\n",
    "\n",
    "print(\"Fitting LightGBM with hyperparameter tuning...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Fit on cleaned training data\n",
    "lgb_grid.fit(X_train_raw_lgb, y_train)\n",
    "\n",
    "# Get the best model\n",
    "lgb_model = lgb_grid.best_estimator_\n",
    "best_params = lgb_grid.best_params_\n",
    "\n",
    "print(f\"Best hyperparameters:\")\n",
    "print(f\"  n_estimators: {best_params['n_estimators']}\")\n",
    "print(f\"  max_depth: {best_params['max_depth']}\")\n",
    "print(f\"  learning_rate: {best_params['learning_rate']}\")\n",
    "print(f\"  subsample: {best_params['subsample']}\")\n",
    "print(f\"  colsample_bytree: {best_params['colsample_bytree']}\")\n",
    "print(f\"  num_leaves: {best_params['num_leaves']}\")\n",
    "print(f\"Cross-validation score: {-lgb_grid.best_score_:.3f}\")\n",
    "\n",
    "# Make predictions using cleaned data\n",
    "y_train_pred = lgb_model.predict(X_train_raw_lgb)\n",
    "y_val_pred = lgb_model.predict(X_val_raw_lgb)\n",
    "\n",
    "# Calculate performance metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nTraining Performance:\")\n",
    "print(f\"  RMSE: {train_rmse:.3f}\")\n",
    "print(f\"  MAE:  {train_mae:.3f}\")\n",
    "print(f\"  R²:   {train_r2:.3f}\")\n",
    "\n",
    "print(f\"\\nValidation Performance:\")\n",
    "print(f\"  RMSE: {val_rmse:.3f}\")\n",
    "print(f\"  MAE:  {val_mae:.3f}\")\n",
    "print(f\"  R²:   {val_r2:.3f}\")\n",
    "\n",
    "# Feature importance for LightGBM\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': predictor_cols_tree,\n",
    "    'importance': lgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6997e6c3",
   "metadata": {},
   "source": [
    "### CatBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fb406db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting CatBoost with hyperparameter tuning...\n",
      "This may take several minutes...\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best hyperparameters:\n",
      "  iterations: 200\n",
      "  depth: 6\n",
      "  learning_rate: 0.1\n",
      "  l2_leaf_reg: 1\n",
      "Cross-validation score: 35.459\n",
      "\n",
      "Training Performance:\n",
      "  RMSE: 2.551\n",
      "  MAE:  1.879\n",
      "  R²:   0.974\n",
      "\n",
      "Validation Performance:\n",
      "  RMSE: 6.377\n",
      "  MAE:  4.554\n",
      "  R²:   0.821\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                 feature  importance\n",
      "6       population_total   13.031898\n",
      "14            region_eap   10.052394\n",
      "19            region_ssa    9.579160\n",
      "2       urban_population    9.163742\n",
      "0         fertility_rate    8.635526\n",
      "1   fertility_adolescent    7.487290\n",
      "8   gdp_per_capita_const    6.228070\n",
      "13  income_level_encoded    6.204995\n",
      "16            region_lac    4.751803\n",
      "3       dependency_ratio    4.583082\n"
     ]
    }
   ],
   "source": [
    "# Note that we use the raw unimputed data for CatBoost because it can handle missing values natively\n",
    "\n",
    "# Clean column names to snake_case for consistency\n",
    "X_train_raw_cat = X_train_raw.copy()\n",
    "X_val_raw_cat = X_val_raw.copy()\n",
    "X_test_raw_cat = X_test_raw.copy()\n",
    "\n",
    "# Convert to snake_case: lowercase, replace special chars and spaces with underscores\n",
    "X_train_raw_cat.columns = (X_train_raw_cat.columns\n",
    "                            .str.lower()\n",
    "                            .str.replace('[^a-z0-9]+', '_', regex=True)\n",
    "                            .str.strip('_'))\n",
    "X_val_raw_cat.columns = (X_val_raw_cat.columns\n",
    "                          .str.lower()\n",
    "                          .str.replace('[^a-z0-9]+', '_', regex=True)\n",
    "                          .str.strip('_'))\n",
    "X_test_raw_cat.columns = (X_test_raw_cat.columns\n",
    "                           .str.lower()\n",
    "                           .str.replace('[^a-z0-9]+', '_', regex=True)\n",
    "                           .str.strip('_'))\n",
    "\n",
    "# Define hyperparameters to test (reduced grid for initial comparison)\n",
    "param_grid = {\n",
    "    'iterations': [100, 200],                 # Number of boosting rounds\n",
    "    'depth': [3, 6],                          # Maximum tree depth\n",
    "    'learning_rate': [0.01, 0.1],             # Step size shrinkage\n",
    "    'l2_leaf_reg': [1, 3, 5]                  # L2 regularization\n",
    "}\n",
    "\n",
    "# Initialize CatBoost with GridSearch for hyperparameter tuning\n",
    "catboost_grid = GridSearchCV(\n",
    "    CatBoostRegressor(random_state=42, verbose=0),\n",
    "    param_grid=param_grid,\n",
    "    cv=time_kfold,  # Time-based CV within training period\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1  # Show progress\n",
    ")\n",
    "\n",
    "print(\"Fitting CatBoost with hyperparameter tuning...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Fit on cleaned training data\n",
    "catboost_grid.fit(X_train_raw_cat, y_train)\n",
    "\n",
    "# Get the best model\n",
    "catboost_model = catboost_grid.best_estimator_\n",
    "best_params = catboost_grid.best_params_\n",
    "\n",
    "print(f\"Best hyperparameters:\")\n",
    "print(f\"  iterations: {best_params['iterations']}\")\n",
    "print(f\"  depth: {best_params['depth']}\")\n",
    "print(f\"  learning_rate: {best_params['learning_rate']}\")\n",
    "print(f\"  l2_leaf_reg: {best_params['l2_leaf_reg']}\")\n",
    "print(f\"Cross-validation score: {-catboost_grid.best_score_:.3f}\")\n",
    "\n",
    "# Make predictions using cleaned data\n",
    "y_train_pred = catboost_model.predict(X_train_raw_cat)\n",
    "y_val_pred = catboost_model.predict(X_val_raw_cat)\n",
    "\n",
    "# Calculate performance metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nTraining Performance:\")\n",
    "print(f\"  RMSE: {train_rmse:.3f}\")\n",
    "print(f\"  MAE:  {train_mae:.3f}\")\n",
    "print(f\"  R²:   {train_r2:.3f}\")\n",
    "\n",
    "print(f\"\\nValidation Performance:\")\n",
    "print(f\"  RMSE: {val_rmse:.3f}\")\n",
    "print(f\"  MAE:  {val_mae:.3f}\")\n",
    "print(f\"  R²:   {val_r2:.3f}\")\n",
    "\n",
    "# Feature importance for CatBoost\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': predictor_cols_tree,\n",
    "    'importance': catboost_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ec949",
   "metadata": {},
   "source": [
    "## Final Evaluation on Test Set\n",
    "Run pre-selected group of best performing models on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c381269e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set performance (2021–2023):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>4.005239</td>\n",
       "      <td>2.284440</td>\n",
       "      <td>0.931236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>7.481277</td>\n",
       "      <td>5.505966</td>\n",
       "      <td>0.760087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVR (RBF)</td>\n",
       "      <td>7.900602</td>\n",
       "      <td>5.464321</td>\n",
       "      <td>0.732439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>8.072678</td>\n",
       "      <td>5.730122</td>\n",
       "      <td>0.720657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>11.874556</td>\n",
       "      <td>9.197685</td>\n",
       "      <td>0.395582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model       rmse       mae        r2\n",
       "2            KNN   4.005239  2.284440  0.931236\n",
       "3  Random Forest   7.481277  5.505966  0.760087\n",
       "1      SVR (RBF)   7.900602  5.464321  0.732439\n",
       "4       CatBoost   8.072678  5.730122  0.720657\n",
       "0    Elastic Net  11.874556  9.197685  0.395582"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_test = []\n",
    "\n",
    "# 1. Elastic Net (uses X_test_scaled)\n",
    "y_test_pred_en = elastic_model.predict(X_test_scaled)\n",
    "results_test.append({\n",
    "    \"model\": \"Elastic Net\",\n",
    "    \"rmse\": np.sqrt(mean_squared_error(y_test, y_test_pred_en)),\n",
    "    \"mae\":  mean_absolute_error(y_test, y_test_pred_en),\n",
    "    \"r2\":   r2_score(y_test, y_test_pred_en),\n",
    "})\n",
    "\n",
    "# 2. SVR (RBF) (uses X_test_scaled)\n",
    "y_test_pred_svr = svr_model.predict(X_test_scaled)\n",
    "results_test.append({\n",
    "    \"model\": \"SVR (RBF)\",\n",
    "    \"rmse\": np.sqrt(mean_squared_error(y_test, y_test_pred_svr)),\n",
    "    \"mae\":  mean_absolute_error(y_test, y_test_pred_svr),\n",
    "    \"r2\":   r2_score(y_test, y_test_pred_svr),\n",
    "})\n",
    "\n",
    "# 3. KNN (uses X_test_scaled)\n",
    "y_test_pred_knn = knn_model.predict(X_test_scaled)\n",
    "results_test.append({\n",
    "    \"model\": \"KNN\",\n",
    "    \"rmse\": np.sqrt(mean_squared_error(y_test, y_test_pred_knn)),\n",
    "    \"mae\":  mean_absolute_error(y_test, y_test_pred_knn),\n",
    "    \"r2\":   r2_score(y_test, y_test_pred_knn),\n",
    "})\n",
    "\n",
    "# 4. Random Forest (uses X_test_imputed)\n",
    "y_test_pred_rf = rf_model.predict(X_test_imputed)\n",
    "results_test.append({\n",
    "    \"model\": \"Random Forest\",\n",
    "    \"rmse\": np.sqrt(mean_squared_error(y_test, y_test_pred_rf)),\n",
    "    \"mae\":  mean_absolute_error(y_test, y_test_pred_rf),\n",
    "    \"r2\":   r2_score(y_test, y_test_pred_rf),\n",
    "})\n",
    "\n",
    "# 5. CatBoost (uses X_test_raw_cat)\n",
    "y_test_pred_cat = catboost_model.predict(X_test_raw_cat)\n",
    "results_test.append({\n",
    "    \"model\": \"CatBoost\",\n",
    "    \"rmse\": np.sqrt(mean_squared_error(y_test, y_test_pred_cat)),\n",
    "    \"mae\":  mean_absolute_error(y_test, y_test_pred_cat),\n",
    "    \"r2\":   r2_score(y_test, y_test_pred_cat),\n",
    "})\n",
    "\n",
    "# Nicely formatted table\n",
    "test_results_df = pd.DataFrame(results_test).sort_values(\"r2\", ascending=False)\n",
    "print(\"Test-set performance (2021–2023):\")\n",
    "display(test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a582462",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Under the country‑based split (where we held out entire countries), all models faced a hard generalization problem and performance was modest, with regularized linear models (Lasso/Elastic Net) and SVR doing best, and tree/boosting models overfitting the training countries and underperforming on validation. When we moved to a temporal 80/10/10 split by year (training on 2000–2018, validating on 2019–2020, testing on 2021–2023), the forecasting task became much easier: all models improved sharply because they now predict later years for countries and regions already seen in the training data. In this setup, the linear models achieved validation R² around 0.40, SVR around 0.73, and the tree/boosting models (Random Forest, XGBoost, LightGBM, CatBoost) reached validation R² in the 0.82–0.87 range.\n",
    "\n",
    "On the held‑out test years, we evaluated a shortlist of models chosen based on temporal CV and validation performance: Elastic Net, SVR (RBF), KNN, Random Forest, and CatBoost. All models showed some drop in performance relative to validation, as expected, but remained reasonably strong. Elastic Net offered a solid linear baseline (test R² ≈ 0.40). SVR, Random Forest, and CatBoost all retained good forecasting accuracy on the test period, with test R² in the low‑to‑mid 0.70s (around 0.72–0.76), and Random Forest emerging as the best of this group (test RMSE ≈ 7.48, R² ≈ 0.76). The standout performer on the test set was KNN, with very low RMSE (≈ 4.0) and very high R² (≈ 0.93), reflecting its strength as a local interpolation method when future observations closely resemble past ones in feature space.\n",
    "\n",
    "Despite KNN’s superior test metrics, we ultimately chose Random Forest as the final model for the app. KNN’s strength here comes from its highly local nature: it effectively averages outcomes of the most similar historical cases, which works extremely well for short‑horizon forecasting of “more of the same” but makes it fragile when asked to predict for unusual or off‑manifold combinations of predictors. The slider‑based app will deliberately create hypothetical countries that may not closely match any real historical observations, and in such regions KNN can behave unpredictably because of the curse of dimensionality and its reliance on distance in a 20+ dimensional feature space. Random Forest, by contrast, learns a more global mapping from predictors to FLFP, extrapolates more smoothly as sliders move, and still delivers strong forecast performance on the temporal test set. For that reason, we treat KNN as a useful upper‑bound benchmark, but adopt Random Forest as the primary model for deployment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
